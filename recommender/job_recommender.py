"""
Syst√®me de recommandation d'offres d'emploi bas√© sur Sentence-BERT et FAISS
"""
import os
import pickle
from pathlib import Path
from typing import List, Dict, Optional, Union
import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import faiss
from tqdm import tqdm

from config import (
    EMBEDDING_MODEL_NAME, EMBEDDING_DIMENSION,
    EMBEDDINGS_PATH, JOBS_PROCESSED_PATH, FAISS_INDEX_PATH,
    SCORING_WEIGHTS, DEFAULT_TOP_K, MAX_TOP_K
)
from data_preprocessing import JobDataPreprocessor, normalize_location
from cv_parser import CVParser


class JobRecommender:
    """
    Syst√®me de recommandation d'offres d'emploi
    
    Utilise Sentence-BERT pour les embeddings s√©mantiques
    et FAISS pour la recherche vectorielle rapide
    """
    
    def __init__(self, force_reload: bool = False):
        """
        Initialise le recommender
        
        Args:
            force_reload: Si True, recharge les embeddings m√™me s'ils existent
        """
        self.preprocessor = JobDataPreprocessor()
        self.cv_parser = CVParser()
        
        print("Initialisation du syst√®me de recommandation...")
        
        # Charger le mod√®le d'embeddings
        print(f"  ‚Üí Chargement du mod√®le: {EMBEDDING_MODEL_NAME}")
        self.model = SentenceTransformer(EMBEDDING_MODEL_NAME)
        
        # Variables pour stocker les donn√©es
        self.jobs_df = None
        self.embeddings = None
        self.faiss_index = None
        
        # Charger ou cr√©er les embeddings
        if not force_reload and self._embeddings_exist():
            self._load_embeddings()
        else:
            self._create_embeddings()
        
        print("Syst√®me de recommandation pr√™t.")
    
    def _embeddings_exist(self) -> bool:
        """V√©rifie si les embeddings existent d√©j√†"""
        return (
            EMBEDDINGS_PATH.exists() and
            JOBS_PROCESSED_PATH.exists() and
            FAISS_INDEX_PATH.exists()
        )
    
    def _create_embeddings(self):
        """Cr√©e les embeddings pour toutes les offres"""
        print("\nCr√©ation des embeddings (cette op√©ration peut prendre quelques minutes)...")
        
        # Charger et pr√©processer les donn√©es
        df_raw = self.preprocessor.load_jobs()
        self.jobs_df = self.preprocessor.preprocess_jobs_df(df_raw)
        
        # G√©n√©rer les embeddings
        print(f"  ‚Üí Vectorisation de {len(self.jobs_df):,} offres...")
        job_texts = self.jobs_df['combined_text'].tolist()
        
        # Encoder par batch pour √©viter les probl√®mes de m√©moire
        self.embeddings = self.model.encode(
            job_texts,
            batch_size=32,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        
        # Cr√©er l'index FAISS
        print("  ‚Üí Construction de l'index FAISS...")
        self._build_faiss_index()
        
        # Sauvegarder
        print("  ‚Üí Sauvegarde des embeddings...")
        self._save_embeddings()
        
        print("Embeddings cr√©√©s et sauvegard√©s.")
    
    def _build_faiss_index(self):
        """Construit l'index FAISS pour la recherche rapide"""
        # Normaliser les embeddings pour utiliser la similarit√© cosinus
        faiss.normalize_L2(self.embeddings)
        
        # Cr√©er un index Flat (exact search) avec similarit√© cosinus
        self.faiss_index = faiss.IndexFlatIP(EMBEDDING_DIMENSION)
        self.faiss_index.add(self.embeddings.astype('float32'))
    
    def _save_embeddings(self):
        """Sauvegarde les embeddings et les donn√©es"""
        # Sauvegarder les embeddings
        np.save(EMBEDDINGS_PATH, self.embeddings)
        
        # Sauvegarder le DataFrame
        with open(JOBS_PROCESSED_PATH, 'wb') as f:
            pickle.dump(self.jobs_df, f)
        
        # Sauvegarder l'index FAISS
        faiss.write_index(self.faiss_index, str(FAISS_INDEX_PATH))
    
    def _load_embeddings(self):
        """Charge les embeddings sauvegard√©s"""
        print("  ‚Üí Chargement des embeddings pr√©-calcul√©s...")
        
        self.embeddings = np.load(EMBEDDINGS_PATH)
        
        with open(JOBS_PROCESSED_PATH, 'rb') as f:
            self.jobs_df = pickle.load(f)
        
        self.faiss_index = faiss.read_index(str(FAISS_INDEX_PATH))
        
        print(f"  ‚Üí {len(self.jobs_df):,} offres charg√©es")
    
    def recommend(
        self,
        candidate_profile: str,
        cv_text: Optional[str] = None,
        keywords: Optional[List[str]] = None,
        location_preference: Optional[str] = None,
        contract_type_preference: Optional[str] = None,
        experience_level: Optional[str] = None,
        top_k: int = DEFAULT_TOP_K,
        min_score: float = 0.0
    ) -> List[Dict]:
        """
        Recommande des offres d'emploi pour un profil candidat
        
        Args:
            candidate_profile: Description textuelle du profil (saisie utilisateur)
            cv_text: Texte extrait du CV (optionnel)
            keywords: Liste de mots-cl√©s/comp√©tences (optionnel)
            location_preference: Localisation pr√©f√©r√©e (optionnel)
            contract_type_preference: Type de contrat pr√©f√©r√© (optionnel)
            experience_level: Niveau d'exp√©rience ('junior', 'mid', 'senior', etc.)
            top_k: Nombre de recommandations √† retourner
            min_score: Score minimum pour filtrer les r√©sultats
            
        Returns:
            Liste de dictionnaires avec les offres recommand√©es et leurs scores
        """
        # Construire le texte complet du candidat
        candidate_text = self._build_candidate_text(
            candidate_profile, cv_text, keywords
        )
        
        # Vectoriser le profil candidat
        candidate_embedding = self.model.encode([candidate_text], convert_to_numpy=True)
        faiss.normalize_L2(candidate_embedding)
        
        # Rechercher les K*2 plus proches voisins (on filtrera apr√®s)
        search_k = min(top_k * 2, len(self.jobs_df))
        distances, indices = self.faiss_index.search(
            candidate_embedding.astype('float32'),
            search_k
        )
        
        # Extraire les comp√©tences du candidat
        candidate_skills = set(self.preprocessor.extract_skills(candidate_text))
        
        # Calculer les scores finaux pour chaque offre
        recommendations = []
        
        for idx, base_score in zip(indices[0], distances[0]):
            job = self.jobs_df.iloc[idx]
            
            # Calcul du score multi-crit√®res
            final_score = self._calculate_final_score(
                base_score=float(base_score),
                job=job,
                candidate_skills=candidate_skills,
                location_preference=location_preference,
                contract_type_preference=contract_type_preference,
                experience_level=experience_level
            )
            
            # Filtrer par score minimum
            if final_score < min_score:
                continue
            
            # Cr√©er l'objet recommandation
            recommendation = {
                'job_id': int(idx),
                'title': job['title'],
                'company': job['companyName'],
                'location': job['location'],
                'contract_type': job['contractType'],
                'work_type': job.get('workType', 'Unknown'),
                'posted_time': job.get('postedTime', 'Unknown'),
                'job_url': job.get('jobUrl', ''),
                'description_preview': job['description_clean'][:300] + '...',
                'skills': job['skills'],
                'experience_level': job['experience_level'],
                'score': round(final_score, 4),
                'semantic_similarity': round(float(base_score), 4),
                'skills_match_count': len(candidate_skills & set(job['skills'])),
                'skills_match_ratio': self._calculate_skills_match_ratio(
                    candidate_skills, set(job['skills'])
                )
            }
            
            recommendations.append(recommendation)
        
        # Trier par score final
        recommendations.sort(key=lambda x: x['score'], reverse=True)
        
        # Retourner top K
        return recommendations[:top_k]
    
    def _build_candidate_text(
        self,
        profile: str,
        cv_text: Optional[str] = None,
        keywords: Optional[List[str]] = None
    ) -> str:
        """Construit le texte complet pour le profil candidat"""
        parts = [profile]
        
        if cv_text:
            parts.append(cv_text)
        
        if keywords:
            # R√©p√©ter les keywords pour leur donner plus de poids
            keywords_text = ' '.join(keywords) + ' ' + ' '.join(keywords)
            parts.append(keywords_text)
        
        return ' '.join(parts)
    
    def _calculate_final_score(
        self,
        base_score: float,
        job: pd.Series,
        candidate_skills: set,
        location_preference: Optional[str],
        contract_type_preference: Optional[str],
        experience_level: Optional[str]
    ) -> float:
        """Calcule le score final avec pond√©ration multi-crit√®res"""
        
        # Score s√©mantique (d√©j√† normalis√© entre 0 et 1)
        semantic_score = base_score
        
        # Score de matching des comp√©tences
        skills_score = self._calculate_skills_match_ratio(
            candidate_skills, set(job['skills'])
        )
        
        # Score de localisation
        location_score = 1.0
        if location_preference:
            pref = location_preference.lower().strip()
            job_loc = str(job['location']).lower().strip()
            
            # Match exact ou inclusion (ex: "Canada" dans "Toronto, Canada")
            if pref in job_loc or job_loc in pref:
                location_score = 1.0
            elif any(keyword in job_loc for keyword in ['remote', 't√©l√©travail', 'distance']):
                # Les postes en remote ont une p√©nalit√© l√©g√®re car ils restent potentiellement accessibles
                location_score = 0.7
            else:
                # Grosse p√©nalit√© pour les localisations qui ne matchent pas (√©vite les jobs US par d√©faut)
                location_score = 0.1
        
        # Score de type de contrat
        contract_score = 1.0
        if contract_type_preference:
            contract_score = 1.0 if contract_type_preference.lower() in job['contractType_clean'].lower() else 0.5
        
        # Score d'exp√©rience am√©lior√©
        experience_score = 1.0
        if experience_level:
            job_level = job.get('experience_level', 'unknown')
            
            levels_order = {'junior': 0, 'mid': 1, 'senior': 2, 'manager': 3}
            
            if experience_level in levels_order and job_level in levels_order:
                cand_idx = levels_order[experience_level]
                job_idx = levels_order[job_level]
                
                # M√™me niveau
                if cand_idx == job_idx:
                    experience_score = 1.0
                # Trop d'exp√©rience pour le poste (ex: Senior postule pour Junior) -> Acceptable mais pas id√©al
                elif cand_idx > job_idx:
                    experience_score = 0.8
                # Pas assez d'exp√©rience (ex: Junior postule pour Senior) -> P√©nalit√©
                else:
                    dist = job_idx - cand_idx
                    experience_score = 0.4 if dist == 1 else 0.1
            else:
                # Si le niveau du job est inconnu, on reste neutre
                experience_score = 0.5
        
        # Calcul du score final pond√©r√©
        final_score = (
            SCORING_WEIGHTS['semantic_similarity'] * semantic_score +
            SCORING_WEIGHTS['skills_match'] * skills_score +
            SCORING_WEIGHTS['location_match'] * location_score +
            SCORING_WEIGHTS['contract_type_match'] * contract_score +
            SCORING_WEIGHTS['experience_match'] * experience_score
        )
        
        return final_score
    
    def _calculate_skills_match_ratio(self, candidate_skills: set, job_skills: set) -> float:
        """Calcule le ratio de correspondance des comp√©tences"""
        if not candidate_skills or not job_skills:
            return 0.0
        
        intersection = len(candidate_skills & job_skills)
        union = len(candidate_skills | job_skills)
        
        # Jaccard similarity
        return intersection / union if union > 0 else 0.0
    
    def recommend_from_cv_file(
        self,
        cv_path: str,
        additional_keywords: Optional[List[str]] = None,
        **kwargs
    ) -> List[Dict]:
        """
        Recommande des offres depuis un fichier CV
        
        Args:
            cv_path: Chemin vers le fichier CV
            additional_keywords: Mots-cl√©s additionnels
            **kwargs: Autres arguments pour recommend()
            
        Returns:
            Liste de recommandations
        """
        # Parser le CV
        cv_text = self.cv_parser.parse_cv(cv_path)
        cv_text = self.cv_parser.clean_text(cv_text)
        
        # Combiner avec les keywords
        all_keywords = additional_keywords or []
        
        return self.recommend(
            candidate_profile="",
            cv_text=cv_text,
            keywords=all_keywords,
            **kwargs
        )
    
    def recommend_from_cv_bytes(
        self,
        cv_bytes: bytes,
        cv_filename: str,
        additional_keywords: Optional[List[str]] = None,
        **kwargs
    ) -> List[Dict]:
        """
        Recommande des offres depuis un CV upload√© (bytes)
        
        Args:
            cv_bytes: Contenu du CV en bytes
            cv_filename: Nom du fichier
            additional_keywords: Mots-cl√©s additionnels
            **kwargs: Autres arguments pour recommend()
            
        Returns:
            Liste de recommandations
        """
        # Parser le CV
        cv_text = self.cv_parser.parse_cv_bytes(cv_bytes, cv_filename)
        cv_text = self.cv_parser.clean_text(cv_text)
        
        all_keywords = additional_keywords or []
        
        return self.recommend(
            candidate_profile="",
            cv_text=cv_text,
            keywords=all_keywords,
            **kwargs
        )
    
    def get_similar_jobs(self, job_id: int, top_k: int = 10) -> List[Dict]:
        """
        Trouve des offres similaires √† une offre donn√©e
        
        Args:
            job_id: ID de l'offre de r√©f√©rence
            top_k: Nombre d'offres similaires √† retourner
            
        Returns:
            Liste d'offres similaires
        """
        if job_id >= len(self.embeddings):
            raise ValueError(f"job_id {job_id} invalide (max: {len(self.embeddings)-1})")
        
        # R√©cup√©rer l'embedding de l'offre
        job_embedding = self.embeddings[job_id:job_id+1]
        
        # Rechercher les similaires (top_k + 1 car le premier sera l'offre elle-m√™me)
        distances, indices = self.faiss_index.search(
            job_embedding.astype('float32'),
            top_k + 1
        )
        
        # Exclure l'offre elle-m√™me et cr√©er les r√©sultats
        similar_jobs = []
        for idx, score in zip(indices[0][1:], distances[0][1:]):
            job = self.jobs_df.iloc[idx]
            similar_jobs.append({
                'job_id': int(idx),
                'title': job['title'],
                'company': job['companyName'],
                'location': job['location'],
                'similarity_score': round(float(score), 4),
                'skills': job['skills']
            })
        
        return similar_jobs
    
    def get_job_details(self, job_id: int) -> Dict:
        """
        R√©cup√®re les d√©tails complets d'une offre
        
        Args:
            job_id: ID de l'offre
            
        Returns:
            Dictionnaire avec tous les d√©tails
        """
        if job_id >= len(self.jobs_df):
            raise ValueError(f"job_id {job_id} invalide")
        
        job = self.jobs_df.iloc[job_id]
        
        return {
            'job_id': int(job_id),
            'title': job['title'],
            'company': job['companyName'],
            'company_url': job.get('companyUrl', ''),
            'location': job['location'],
            'contract_type': job['contractType'],
            'work_type': job.get('workType', ''),
            'posted_time': job.get('postedTime', ''),
            'published_at': job.get('publishedAt', ''),
            'job_url': job.get('jobUrl', ''),
            'description': job['description'],
            'skills': job['skills'],
            'num_skills': job['num_skills'],
            'experience_level': job['experience_level'],
            'years_experience': job['years_experience']
        }
    
    def get_statistics(self) -> Dict:
        """Retourne des statistiques sur les offres"""
        return self.preprocessor.get_statistics(self.jobs_df)


if __name__ == "__main__":
    # Test du recommender
    print("\n" + "="*80)
    print("TEST DU SYST√àME DE RECOMMANDATION")
    print("="*80)
    
    # Initialiser
    recommender = JobRecommender()
    
    # Exemple de profil candidat
    candidate_profile = """
    Data Scientist avec 3 ans d'exp√©rience en Machine Learning et Python.
    Comp√©tences: Python, scikit-learn, TensorFlow, SQL, Pandas, NumPy.
    Recherche poste en r√©gion parisienne, full-time.
    """
    
    keywords = ['Python', 'Machine Learning', 'Data Science', 'SQL', 'TensorFlow']
    
    # Obtenir des recommandations
    print("\nüîç Recherche de recommandations...")
    recommendations = recommender.recommend(
        candidate_profile=candidate_profile,
        keywords=keywords,
        location_preference="Paris",
        contract_type_preference="Full-time",
        top_k=5
    )
    
    # Afficher les r√©sultats
    print(f"\nüìã Top {len(recommendations)} recommandations:\n")
    
    for i, rec in enumerate(recommendations, 1):
        print(f"{i}. {rec['title']} - {rec['company']}")
        print(f"   üìç {rec['location']} | üìÑ {rec['contract_type']}")
        print(f"   üéØ Score: {rec['score']:.3f} | Comp√©tences match√©es: {rec['skills_match_count']}")
        print(f"   üíº {', '.join(rec['skills'][:5])}...")
        print()
